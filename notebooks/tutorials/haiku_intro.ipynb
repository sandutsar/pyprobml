{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "haiku_intro.ipynb",
   "provenance": [],
   "toc_visible": true,
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyORxANOTQ+dBTVqbMOqM4e1",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "190095d5de254f0f92add36689a2f37c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_af7c5fbbc4324da9b459ab5a214f2eed",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_ea6a64028530455385fbf0faddf33934",
       "IPY_MODEL_b4e39f75f93a4e49852b027c96a45261",
       "IPY_MODEL_67cfa9b71c2245dcad60c57eccdf3882"
      ]
     }
    },
    "af7c5fbbc4324da9b459ab5a214f2eed": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "ea6a64028530455385fbf0faddf33934": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_58b64255af8347fc95ddea22107f7ae9",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": "Dl Completed...: 100%",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_a4f93609a2c54ab5a0ca979a557ee62f"
     }
    },
    "b4e39f75f93a4e49852b027c96a45261": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_3a1114b5331e4d7f8874acad1011f08e",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 4,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 4,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_40709ad839df4e2898e5138e2896e645"
     }
    },
    "67cfa9b71c2245dcad60c57eccdf3882": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_308b203c34054dfabd2e9fcdb35604f9",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 4/4 [00:01&lt;00:00,  1.95 file/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_2a3d89b100f24c6790fec5ef5c0be25e"
     }
    },
    "58b64255af8347fc95ddea22107f7ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "a4f93609a2c54ab5a0ca979a557ee62f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "3a1114b5331e4d7f8874acad1011f08e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "40709ad839df4e2898e5138e2896e645": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "308b203c34054dfabd2e9fcdb35604f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "2a3d89b100f24c6790fec5ef5c0be25e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/probml-notebooks/blob/main/notebooks/haiku_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbJqnWzn9L6S"
   },
   "source": [
    "# An introduction to haiku (neural network library in JAX)\n",
    "\n",
    "https://github.com/deepmind/dm-haiku\n",
    "\n",
    "Haiku is a JAX version of the [Sonnet](https://github.com/deepmind/sonnet) neural network library (which was written in Tensorflow2). The main thing it does is to provide a way to convert object-oriented (stateful) code into functionally pure code, which can then be processed by JAX transformations like jit and grad. In addition it has implementations of common neural net building blocks.\n",
    "\n",
    "Below we give a brief introduction, based on the offical docs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yi0DMWY89LKZ"
   },
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/deepmind/dm-haiku\n",
    "import haiku as hk"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xDcW57Ynd5jX"
   },
   "source": [
    "%%capture\n",
    "!pip install git+git://github.com/deepmind/optax.git\n",
    "import optax"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JpbH4Hhr9N_O"
   },
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JOlff_nHlz6"
   },
   "source": [
    "# Haiku function transformations\n",
    "\n",
    "The main thing haiku offers is a way to let the user write a function that defines and accesses mutable parameters inside the function, and then to transform this into a function that takes the parameters as explicit arguments. (The advantage of the implicit method will become clearer later, when we consider modules, which let the user define parameters using nested objects.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e9-i6pOFIPFL"
   },
   "source": [
    "# Here is a function that takes in data x, and meta-data output_size,\n",
    "# but creates its mutable parameters internally.\n",
    "# The parameters define an affine mapping, f1(x) = b + W*x\n",
    "def f1(x, output_size):\n",
    "    j, k = x.shape[-1], output_size\n",
    "    w_init = hk.initializers.TruncatedNormal(1.0 / np.sqrt(j))\n",
    "    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
    "    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
    "    return jnp.dot(x, w) + b"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Y6HcwKyJyIB",
    "outputId": "638ec341-1c5a-4a8f-fbf6-e6b597aa6f2f"
   },
   "source": [
    "# transform will convert f1 to a function that explicitly uses parameters, which we call f2.\n",
    "# (We explain the rng part later.)\n",
    "f2 = hk.without_apply_rng(hk.transform(f1))\n",
    "\n",
    "# f2 is a struct with two functions, init and apply\n",
    "print(f2)"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transformed(init=<function without_state.<locals>.init_fn at 0x7fbe71410320>, apply=<function without_apply_rng.<locals>.apply_fn at 0x7fbe714105f0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbXmx9yYI9fJ",
    "outputId": "305dda0b-a415-4ff4-912b-e1c07823557b"
   },
   "source": [
    "# The init function creates an initial random set of parameters\n",
    "# by calling f1 on some data x (the values don't matter, just the shape)\n",
    "# and using the RNG.\n",
    "# The params are stoerd in a haiku FlatMap (like a FrozenDict)\n",
    "output_size = 2\n",
    "dummy_x = jnp.array([[1.0, 2.0, 3.0]])\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "# params = f2.init(rng=rng_key, x=dummy_x, output_size = output_size)\n",
    "params = f2.init(rng_key, dummy_x, output_size)\n",
    "print(params)"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FlatMap({\n",
      "  '~': FlatMap({\n",
      "         'w': DeviceArray([[-0.30350363,  0.5123802 ],\n",
      "                           [ 0.08009141, -0.3163005 ],\n",
      "                           [ 0.6056666 ,  0.58207023]], dtype=float32),\n",
      "         'b': DeviceArray([1., 1.], dtype=float32),\n",
      "       }),\n",
      "})\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxcFt7GiKocY",
    "outputId": "3883cca5-9621-4ec6-a798-86a88c57a7e7"
   },
   "source": [
    "p = params[\"~\"]\n",
    "print(p[\"b\"])"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "5X2uAetOLcpU",
    "outputId": "d0f1fd5b-b2b4-4e0f-b743-b8f614eb39b0"
   },
   "source": [
    "# params are frozen\n",
    "params[\"~\"][\"b\"] = jnp.array([2.0, 2.0])"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c3f935c6e770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'~'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'FlatMap' object does not support item assignment"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hmEWyvtQKBJD",
    "outputId": "f21f0396-738d-47b5-a76d-c4be8c28f9e0"
   },
   "source": [
    "# The apply function takes a param FlatMap and injects it into the original f1 function\n",
    "sample_x = jnp.array([[1.0, 2.0, 3.0]])\n",
    "output_1 = f2.apply(params=params, x=sample_x, output_size=output_size)\n",
    "print(output_1)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2.6736789 2.62599  ]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KVtU8v2Q9NG"
   },
   "source": [
    "# Transforming stateful functions\n",
    "\n",
    "We can create a function with internal state that is mutated on each call,\n",
    "but is treated separately from the fixed parameters (which are usually mutated by an external optimizer). Below we illustrate this for a simple counter example, that gets incremented on each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXCW0UNSRZaF",
    "outputId": "383dc9d0-8f1e-4d70-f6bf-ddd464d74683"
   },
   "source": [
    "def stateful_f(x):\n",
    "    counter = hk.get_state(\"counter\", shape=[], dtype=jnp.int32, init=jnp.ones)\n",
    "    multiplier = hk.get_parameter(\n",
    "        \"multiplier\",\n",
    "        shape=[\n",
    "            1,\n",
    "        ],\n",
    "        dtype=x.dtype,\n",
    "        init=jnp.ones,\n",
    "    )\n",
    "    hk.set_state(\"counter\", counter + 1)\n",
    "    output = x + multiplier * counter\n",
    "    return output\n",
    "\n",
    "\n",
    "stateful_forward = hk.without_apply_rng(hk.transform_with_state(stateful_f))\n",
    "sample_x = jnp.array(\n",
    "    [\n",
    "        [\n",
    "            5.0,\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "params, state = stateful_forward.init(x=sample_x, rng=rng_key)\n",
    "print(f\"Initial params:\\n{params}\\nInitial state:\\n{state}\")\n",
    "print(\"##########\")\n",
    "for i in range(3):\n",
    "    output, state = stateful_forward.apply(params, state, x=sample_x)\n",
    "    print(f\"After {i+1} iterations:\\nOutput: {output}\\nState: {state}\")\n",
    "    print(\"##########\")"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial params:\n",
      "FlatMap({'~': FlatMap({'multiplier': DeviceArray([1.], dtype=float32)})})\n",
      "Initial state:\n",
      "FlatMap({'~': FlatMap({'counter': DeviceArray(1, dtype=int32)})})\n",
      "##########\n",
      "After 1 iterations:\n",
      "Output: [[6.]]\n",
      "State: FlatMap({'~': FlatMap({'counter': DeviceArray(2, dtype=int32)})})\n",
      "##########\n",
      "After 2 iterations:\n",
      "Output: [[7.]]\n",
      "State: FlatMap({'~': FlatMap({'counter': DeviceArray(3, dtype=int32)})})\n",
      "##########\n",
      "After 3 iterations:\n",
      "Output: [[8.]]\n",
      "State: FlatMap({'~': FlatMap({'counter': DeviceArray(4, dtype=int32)})})\n",
      "##########\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwvRhDUlLsSq"
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jDgDevPNTPS"
   },
   "source": [
    "Creating a single dict of parameters and passing it as an argument is easy,\n",
    "and haiku is overkill for such cases. However we often have nested parameterized functions, each of which has metadata (like `output_sizes` above) that needs to specified. In such cases it is easier to work with haiku modules. These are just like regular Python classes (no required methods), but typically have a `__init__` constructor and a `__call__` method that can be invoked when calling the module. Below we reimplement the affine function f1 as a module.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9ySaGWV3IFDX"
   },
   "source": [
    "class MyLinear1(hk.Module):\n",
    "    def __init__(self, output_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        j, k = x.shape[-1], self.output_size\n",
    "        w_init = hk.initializers.TruncatedNormal(1.0 / np.sqrt(j))\n",
    "        w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
    "        b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
    "        return jnp.dot(x, w) + b"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "maCs7NjiIIWW"
   },
   "source": [
    "def _forward_fn_linear1(x):\n",
    "    module = MyLinear1(output_size=2)\n",
    "    return module(x)\n",
    "\n",
    "\n",
    "forward_linear1 = hk.without_apply_rng(hk.transform(_forward_fn_linear1))"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3SUOKgTdIqPk",
    "outputId": "571be59f-bb55-420a-af4c-604999ccafab"
   },
   "source": [
    "dummy_x = jnp.array([[1.0, 2.0, 3.0]])\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "\n",
    "params = forward_linear1.init(rng=rng_key, x=dummy_x)\n",
    "print(params)\n",
    "\n",
    "sample_x = jnp.array([[1.0, 2.0, 3.0]])\n",
    "\n",
    "output_1 = forward_linear1.apply(params=params, x=sample_x)\n",
    "print(output_1)"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FlatMap({\n",
      "  'my_linear1': FlatMap({\n",
      "                  'w': DeviceArray([[-0.30350363,  0.5123802 ],\n",
      "                                    [ 0.08009141, -0.3163005 ],\n",
      "                                    [ 0.6056666 ,  0.58207023]], dtype=float32),\n",
      "                  'b': DeviceArray([1., 1.], dtype=float32),\n",
      "                }),\n",
      "})\n",
      "[[2.6736789 2.62599  ]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2eSwM2ER1tA"
   },
   "source": [
    "# Nested and built-in modules\n",
    "\n",
    "We can nest modules inside of each other. This allows us to create complex functions. Haiku ships with [many common layers](https://dm-haiku.readthedocs.io/en/latest/api.html#common-modules), as well as a \n",
    "[small number of common models](https://dm-haiku.readthedocs.io/en/latest/api.html#module-haiku.nets), like MLPs and Resnets. (A model is just multiple layers.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dLnlxW-I2jh",
    "outputId": "50175ddc-dfbc-4d60-9dd0-1d62debdf320"
   },
   "source": [
    "class MyModuleCustom(hk.Module):\n",
    "    def __init__(self, output_size=2, name=\"custom_linear\"):\n",
    "        super().__init__(name=name)\n",
    "        self._internal_linear_1 = hk.nets.MLP(output_sizes=[2, 3], name=\"hk_internal_linear\")\n",
    "        self._internal_linear_2 = MyLinear1(output_size=output_size, name=\"old_linear\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self._internal_linear_2(self._internal_linear_1(x))\n",
    "\n",
    "\n",
    "def _custom_forward_fn(x):\n",
    "    module = MyModuleCustom()\n",
    "    return module(x)\n",
    "\n",
    "\n",
    "custom_forward_without_rng = hk.without_apply_rng(hk.transform(_custom_forward_fn))\n",
    "params = custom_forward_without_rng.init(rng=rng_key, x=sample_x)\n",
    "params"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FlatMap({\n",
       "  'custom_linear/~/hk_internal_linear/~/linear_0': FlatMap({\n",
       "                                                     'w': DeviceArray([[ 1.51595   , -0.23353335]], dtype=float32),\n",
       "                                                     'b': DeviceArray([0., 0.], dtype=float32),\n",
       "                                                   }),\n",
       "  'custom_linear/~/hk_internal_linear/~/linear_1': FlatMap({\n",
       "                                                     'w': DeviceArray([[-0.22075887, -0.27375957,  0.5931483 ],\n",
       "                                                                       [ 0.78180677,  0.72626334, -0.6860752 ]], dtype=float32),\n",
       "                                                     'b': DeviceArray([0., 0., 0.], dtype=float32),\n",
       "                                                   }),\n",
       "  'custom_linear/~/old_linear': FlatMap({\n",
       "                                  'w': DeviceArray([[ 0.28584382,  0.31626168],\n",
       "                                                    [ 0.23357749, -0.4827032 ],\n",
       "                                                    [-0.14647584, -0.7185701 ]], dtype=float32),\n",
       "                                  'b': DeviceArray([1., 1.], dtype=float32),\n",
       "                                }),\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57eKZjlNSGlW"
   },
   "source": [
    "# Stochastic modules\n",
    "\n",
    "\n",
    "If the module is stochastic, we have to pass the RNG to the apply function (as well as the init function), as we show below. We can use `hk.next_rng_key()` to derive a new key from the one that the user passes to `apply`. This is useful for when we have nested modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZhc9ttJSIP7",
    "outputId": "58338735-523d-43d4-d973-2d6f69bcfada"
   },
   "source": [
    "class HkRandom2(hk.Module):\n",
    "    def __init__(self, rate=0.5):\n",
    "        super().__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def __call__(self, x):\n",
    "        key1 = hk.next_rng_key()\n",
    "        return jax.random.bernoulli(key1, 1.0 - self.rate, shape=x.shape)\n",
    "\n",
    "\n",
    "class HkRandomNest(hk.Module):\n",
    "    def __init__(self, rate=0.5):\n",
    "        super().__init__()\n",
    "        self.rate = rate\n",
    "        self._another_random_module = HkRandom2()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        key2 = hk.next_rng_key()\n",
    "        p1 = self._another_random_module(x)\n",
    "        p2 = jax.random.bernoulli(key2, 1.0 - self.rate, shape=x.shape)\n",
    "        print(f\"Bernoullis are  : {p1, p2}\")\n",
    "\n",
    "\n",
    "# Note that the modules that are stochastic cannot be wrapped with hk.without_apply_rng()\n",
    "forward = hk.transform(lambda x: HkRandomNest()(x))\n",
    "\n",
    "x = jnp.array(1.0)\n",
    "params = forward.init(rng_key, x=x)\n",
    "# The 2 Bernoullis can be difference, since they use key1 and key2\n",
    "# But across the 5 iterations the answers should be the same,\n",
    "# since they are all produced by passing in the same rng_key to apply.\n",
    "for i in range(5):\n",
    "    print(f\"\\n Iteration {i+1}\")\n",
    "    prediction = forward.apply(params, x=x, rng=rng_key)"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
      "\n",
      " Iteration 1\n",
      "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
      "\n",
      " Iteration 2\n",
      "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
      "\n",
      " Iteration 3\n",
      "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
      "\n",
      " Iteration 4\n",
      "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
      "\n",
      " Iteration 5\n",
      "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3EaBFJ9TP2C"
   },
   "source": [
    "# Combining JAX Function transformations and Haiku\n",
    "\n",
    "We cannot apply JAX function transformations, like jit and grad, inside of a haiku module, since modules are impure. So we have to use `hk.jit`, `hk.grad`, etc.\n",
    "See [this page](https://dm-haiku.readthedocs.io/en/latest/notebooks/transforms.html) for details. However, after transforming the haiku code to be pure, we can apply JAX transformations as usual.\n",
    "\n",
    "\n",
    "(See also the [equinox libary](https://github.com/patrick-kidger/equinox) for an alternative approach to this problem.)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "42HeSZhgTeHF"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt7ArYx7am0T"
   },
   "source": [
    "# Example: MLP on MNIST\n",
    "\n",
    "This example is modified from https://github.com/deepmind/dm-haiku/blob/main/examples/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jxr-Yn5Xaq-P"
   },
   "source": [
    "from typing import Generator, Mapping, Tuple\n",
    "\n",
    "from absl import app\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "Batch = Mapping[str, np.ndarray]"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "190095d5de254f0f92add36689a2f37c",
      "af7c5fbbc4324da9b459ab5a214f2eed",
      "ea6a64028530455385fbf0faddf33934",
      "b4e39f75f93a4e49852b027c96a45261",
      "67cfa9b71c2245dcad60c57eccdf3882",
      "58b64255af8347fc95ddea22107f7ae9",
      "a4f93609a2c54ab5a0ca979a557ee62f",
      "3a1114b5331e4d7f8874acad1011f08e",
      "40709ad839df4e2898e5138e2896e645",
      "308b203c34054dfabd2e9fcdb35604f9",
      "2a3d89b100f24c6790fec5ef5c0be25e"
     ]
    },
    "id": "r9u3nVb8bUGj",
    "outputId": "756e1cf2-10ac-41dc-d9e6-8b2933625dc9"
   },
   "source": [
    "# Data\n",
    "def load_dataset(\n",
    "    split: str,\n",
    "    *,\n",
    "    is_training: bool,\n",
    "    batch_size: int,\n",
    ") -> Generator[Batch, None, None]:\n",
    "    \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
    "    ds = tfds.load(\"mnist:3.*.*\", split=split).cache().repeat()\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(10 * batch_size, seed=0)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return iter(tfds.as_numpy(ds))\n",
    "\n",
    "\n",
    "# Make datasets.\n",
    "train = load_dataset(\"train\", is_training=True, batch_size=1000)\n",
    "train_eval = load_dataset(\"train\", is_training=False, batch_size=10000)\n",
    "test_eval = load_dataset(\"test\", is_training=False, batch_size=10000)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190095d5de254f0f92add36689a2f37c",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LkgyFvG0a07d"
   },
   "source": [
    "# Model\n",
    "NCLASSES = 10\n",
    "\n",
    "\n",
    "def net_fn(batch: Batch) -> jnp.ndarray:\n",
    "    \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n",
    "    x = batch[\"image\"].astype(jnp.float32) / 255.0\n",
    "    mlp = hk.Sequential(\n",
    "        [\n",
    "            hk.Flatten(),\n",
    "            hk.Linear(300),\n",
    "            jax.nn.relu,\n",
    "            hk.Linear(100),\n",
    "            jax.nn.relu,\n",
    "            hk.Linear(NCLASSES),\n",
    "        ]\n",
    "    )\n",
    "    return mlp(x)\n",
    "\n",
    "\n",
    "net = hk.without_apply_rng(hk.transform(net_fn))\n",
    "L2_REGULARIZER = 1e-4"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "faFP5Efga_zZ"
   },
   "source": [
    "# Metrics\n",
    "\n",
    "# Training loss (cross-entropy).\n",
    "def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "    \"\"\"Compute the loss of the network, including L2.\"\"\"\n",
    "    logits = net.apply(params, batch)\n",
    "    labels = jax.nn.one_hot(batch[\"label\"], NCLASSES)\n",
    "\n",
    "    l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n",
    "    softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
    "    softmax_xent /= labels.shape[0]\n",
    "\n",
    "    return softmax_xent + L2_REGULARIZER * l2_loss\n",
    "\n",
    "\n",
    "# Evaluation metric (classification accuracy).\n",
    "@jax.jit\n",
    "def accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "    predictions = net.apply(params, batch)\n",
    "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch[\"label\"])\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params,\n",
    "    opt_state: optax.OptState,\n",
    "    batch: Batch,\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "    grads = jax.grad(loss)(params, batch)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state\n",
    "\n",
    "\n",
    "# We maintain avg_params, the exponential moving average of the \"live\" params.\n",
    "# avg_params is used only for evaluation (cf. https://doi.org/10.1137/0330046)\n",
    "@jax.jit\n",
    "def ema_update(params, avg_params):\n",
    "    return optax.incremental_update(params, avg_params, step_size=0.001)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3--0utzbQze",
    "outputId": "a2383453-27c4-4735-ed57-e19a5cc2824b"
   },
   "source": [
    "# Optimzier\n",
    "\n",
    "LR = 1e-3\n",
    "opt = optax.adam(LR)\n",
    "\n",
    "# Initialize network and optimiser; note we draw an input to get shapes.\n",
    "params = avg_params = net.init(jax.random.PRNGKey(42), next(train))\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "# Train/eval loop.\n",
    "nsteps = 500\n",
    "print_every = 100\n",
    "\n",
    "\n",
    "def callback(step, avg_params, train_eval, test_eval):\n",
    "    if step % print_every == 0:\n",
    "        # Periodically evaluate classification accuracy on train & test sets.\n",
    "        train_accuracy = accuracy(avg_params, next(train_eval))\n",
    "        test_accuracy = accuracy(avg_params, next(test_eval))\n",
    "        train_accuracy, test_accuracy = jax.device_get((train_accuracy, test_accuracy))\n",
    "        print(f\"[Step {step}] Train / Test accuracy: \" f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
    "\n",
    "\n",
    "for step in range(nsteps + 1):\n",
    "    params, opt_state = update(params, opt_state, next(train))\n",
    "    avg_params = ema_update(params, avg_params)\n",
    "    callback(step, avg_params, train_eval, test_eval)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Step 0] Train / Test accuracy: 0.129 / 0.132.\n",
      "[Step 100] Train / Test accuracy: 0.544 / 0.544.\n",
      "[Step 200] Train / Test accuracy: 0.802 / 0.809.\n",
      "[Step 300] Train / Test accuracy: 0.887 / 0.884.\n",
      "[Step 400] Train / Test accuracy: 0.919 / 0.919.\n",
      "[Step 500] Train / Test accuracy: 0.941 / 0.937.\n"
     ]
    }
   ]
  }
 ]
}