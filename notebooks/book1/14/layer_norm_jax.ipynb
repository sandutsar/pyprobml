{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a650e6fd",
   "metadata": {},
   "source": [
    "Please find torch implementation of this notebook here: https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/book1/14/layer_norm_torch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pxxCjM4AIsZP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "try:\n",
    "    from flax import linen as nn\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq flax\n",
    "    from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcFY2bZeImQI",
    "outputId": "88ca947c-d827-49b5-bc72-af544642a046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch norm\n",
      "[[-1.  1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "layer norm\n",
      "[[ 0.47376014 -1.39085732  0.91709718]\n",
      " [ 1.41421356 -0.70711669 -0.70709687]]\n"
     ]
    }
   ],
   "source": [
    "# batch size 3, feature size 2\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(size=(2, 3))\n",
    "\n",
    "print(\"batch norm\")\n",
    "mu_batch = np.mean(X, axis=0)\n",
    "sigma_batch = np.std(X, axis=0)\n",
    "XBN = (X - mu_batch) / sigma_batch\n",
    "print(XBN)\n",
    "\n",
    "print(\"layer norm\")\n",
    "mu_layer = np.expand_dims(np.mean(X, axis=1), axis=1)\n",
    "sigma_layer = np.expand_dims(np.std(X, axis=1), axis=1)\n",
    "XLN = (X - mu_layer) / sigma_layer\n",
    "print(XLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6p9f1kSyJbuT",
    "outputId": "e69ec5a1-b554-4d20-8f4d-a95665437165"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch norm\n",
      "[[-0.99999815  0.99978346  0.99999744]\n",
      " [ 0.99999815 -0.9997831  -0.9999975 ]]\n",
      "layer norm\n",
      "[[ 0.473758   -1.3908514   0.9170933 ]\n",
      " [ 1.4142125  -0.70711625 -0.7070964 ]]\n"
     ]
    }
   ],
   "source": [
    "X = jnp.float32(X)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "bn_rng, ln_rng = jax.random.split(rng)\n",
    "\n",
    "print(\"batch norm\")\n",
    "bn = nn.BatchNorm(use_running_average=False, epsilon=1e-6)\n",
    "bn_params = bn.init(bn_rng, X)\n",
    "XBN_t, _ = bn.apply(bn_params, X, mutable=[\"batch_stats\"])\n",
    "print(XBN_t)\n",
    "assert np.allclose(np.array(XBN_t), XBN, atol=1e-3)\n",
    "\n",
    "print(\"layer norm\")\n",
    "ln = nn.LayerNorm()\n",
    "ln_params = ln.init(ln_rng, X)\n",
    "XLN_t = ln.apply(ln_params, X)\n",
    "print(XLN_t)\n",
    "assert np.allclose(np.array(XLN_t), XLN, atol=1e-3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "layer-norm-jax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
