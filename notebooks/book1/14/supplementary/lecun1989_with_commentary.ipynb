{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q9JZjt16AMf"
   },
   "source": [
    "# Annotated MNIST\n",
    "\n",
    "This tutorial demonstrates how to construct the original convolutional neural\n",
    "network (CNN) proposed by LeCun et al. in http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf\n",
    "\n",
    "The original pytorch tutorial is at https://github.com/karpathy/lecun1989-repro/blob/master/prepro.py.\n",
    "\n",
    "It is converted to use JAX/ [Flax](https://flax.readthedocs.io), and is based on Flax's official\n",
    "[Annotated MNIST](https://flax.readthedocs.io/en/latest/notebooks/annotated_mnist.html) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSaA8Mif6srP"
   },
   "source": [
    "## 1. Imports\n",
    "\n",
    "Import JAX, \n",
    "Flax, ordinary NumPy, and torchvision datasets. Flax can use any\n",
    "data-loading pipeline and this example demonstrates how to utilize torchvision datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inJ9bV636dRx"
   },
   "outputs": [],
   "source": [
    "!pip install -q flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7MuGFB16E8m"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "\n",
    "from flax import linen as nn  # The Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "import numpy as np  # Ordinary NumPy\n",
    "import optax  # Optimizers\n",
    "from torchvision import datasets  # torchvision.datasets For MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0FW1DHa6cfH"
   },
   "source": [
    "## 2. Define network\n",
    "\n",
    "Create the original convolutional neural network with the Linen API by subclassing\n",
    "[Module](https://flax.readthedocs.io/en/latest/flax.linen.html#core-module-abstraction).\n",
    "Because the architecture in this example is fairly complex—the connection between the first and second hidden layers is quite unusual from a modern point of view—you cannot define the inlined submodules directly within the\n",
    "`__call__` method and wrap it with the\n",
    "[@compact](https://flax.readthedocs.io/en/latest/flax.linen.html#compact-methods)\n",
    "decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most notable difference between LeCun1989 and recent CNNs is that the \"units\" in the original architecture share their weights but do not share their biases (thresholds), whereas its modern descendants share both weights and biases between the units. We define a custom `LocalBias` layer to capture this particularity."
   ],
   "metadata": {
    "id": "j7vu46Bz4upw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LocalBias(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        bias_shape = x.shape[1:]\n",
    "        bias = self.param(\"bias\", nn.initializers.zeros, bias_shape, jnp.float32)\n",
    "        bias = jnp.asarray(bias, jnp.float32)\n",
    "        bias = bias.reshape((1,) * (x.ndim - bias.ndim) + bias.shape)\n",
    "        return x + bias"
   ],
   "metadata": {
    "id": "40HZUi9M4n6b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to write our own `__call__` function. In particular, H2 neurons all connect to only 8 of the 12 input planes. We implement this with 3 separate convolutions that we concatenate the results of. Additionally, we define a custom weight-initializing function `lecun1989_uniform` and a static method `pad` to pad images with `-1` on the edges."
   ],
   "metadata": {
    "id": "Fj3a63tg4syf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_s1lXBBO66dc"
   },
   "outputs": [],
   "source": [
    "class LeCun1989(nn.Module):\n",
    "    \"\"\"1989 LeCun ConvNet per description in the paper\"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        # The variance of Uniform[-2.4/sqrt(fan_in), 2.4/sqrt(fan_in)]\n",
    "        # is 2.4**2/3/fan_in\n",
    "        lecun1989_uniform = jax.nn.initializers.variance_scaling(2.4**2 / 3, \"fan_in\", \"uniform\")\n",
    "        self.H1w = nn.Conv(12, (5, 5), 2, use_bias=False, kernel_init=lecun1989_uniform, padding=\"VALID\")\n",
    "        self.H1b = LocalBias()\n",
    "\n",
    "        # Each slice look at 8 planes and output 4 planes, so when we\n",
    "        # concatenate the output planes together, we get a total of 12 planes.\n",
    "        self.H2s1w = nn.Conv(4, (5, 5), 2, use_bias=False, kernel_init=lecun1989_uniform, padding=\"VALID\")\n",
    "        self.H2s2w = nn.Conv(4, (5, 5), 2, use_bias=False, kernel_init=lecun1989_uniform, padding=\"VALID\")\n",
    "        self.H2s3w = nn.Conv(4, (5, 5), 2, use_bias=False, kernel_init=lecun1989_uniform, padding=\"VALID\")\n",
    "        self.H2b = LocalBias()\n",
    "\n",
    "        self.H3 = nn.Dense(30, kernel_init=lecun1989_uniform)\n",
    "\n",
    "        self.Out = nn.Dense(10, kernel_init=lecun1989_uniform, bias_init=jax.nn.initializers.constant(-1))\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(x):\n",
    "        return jnp.pad(x, ((0, 0), (2, 2), (2, 2), (0, 0)), constant_values=-1.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.H1w(x)\n",
    "        x = self.H1b(x)\n",
    "        x = jnp.tanh(x)\n",
    "\n",
    "        x1 = self.pad(x[..., 0:8])\n",
    "        x2 = self.pad(x[..., 2:10])\n",
    "        x3 = self.pad(x[..., 4:12])\n",
    "        x = jnp.concatenate([self.H2s1w(x1), self.H2s2w(x2), self.H2s3w(x3)], axis=-1)\n",
    "        x = self.H2b(x)\n",
    "        x = jnp.tanh(x)\n",
    "\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = self.H3(x)\n",
    "        x = jnp.tanh(x)\n",
    "\n",
    "        x = self.Out(x)\n",
    "        x = jnp.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDEoAprU6_JZ"
   },
   "source": [
    "## 3. Define loss\n",
    "\n",
    "Define a cross-entropy loss function using just\n",
    "[jax.numpy](https://jax.readthedocs.io/en/latest/jax.numpy.html)\n",
    "that takes the model's logits and label vectors and returns a scalar loss. The\n",
    "labels can be one-hot encoded with\n",
    "[jax.nn.one_hot](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.one_hot.html),\n",
    "as demonstrated below.\n",
    "\n",
    "Note that for demonstration purposes, we return `nn.log_softmax()` from\n",
    "the model and then simply multiply these (normalized) logits with the labels. In our\n",
    "`examples/mnist` folder we actually return non-normalized logits and then use\n",
    "`optax.softmax_cross_entropy()` to compute the loss, which has the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zcb_ebU87G7s"
   },
   "outputs": [],
   "source": [
    "def mse_loss(*, logits, labels):\n",
    "    one_hot_labels = 2 * jax.nn.one_hot(labels, num_classes=10) - 1\n",
    "    return jnp.mean((logits - one_hot_labels) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INZE3eM67JUr"
   },
   "source": [
    "## 4. Metric computation\n",
    "\n",
    "For loss and accuracy metrics, create a separate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvuEA8Tw-MYa"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "    loss = mse_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYz0Emry-ele"
   },
   "source": [
    "## 5. Loading data\n",
    "\n",
    "Define a function that loads and prepares the MNIST dataset and converts the\n",
    "samples to floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOeWiS_b-p8O"
   },
   "outputs": [],
   "source": [
    "def get_dateset(train: bool, size: int):\n",
    "    data = datasets.MNIST(\"/tmp/mnist\", train=train, download=True)\n",
    "\n",
    "    X = data.data[:size].float() / 127.5 - 1.0\n",
    "    X = jnp.float32(X)\n",
    "    X = jax.image.resize(X, (size, 16, 16), \"bilinear\")\n",
    "    X = jnp.expand_dims(X, 3)\n",
    "\n",
    "    Y = jnp.float32(data.targets[:size])\n",
    "\n",
    "    return {\"image\": X, \"label\": Y}\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "    \"\"\"Preprocess today's MNIST dataset into 1989 version's size/format (approximately)\n",
    "\n",
    "    Some relevant notes for this part:\n",
    "    - First 7291 digits from the training set are used for training\n",
    "    - First 2007 digits from the testing set are used for testing\n",
    "    - each image is 16x16 pixels grayscale (not binary)\n",
    "    - images are scaled to range [-1, 1]\n",
    "    - paper doesn't say exactly, but reading between the lines I assume label targets to be {-1, 1}\n",
    "\n",
    "    >>> from contextlib import redirect_stdout\n",
    "    >>> with redirect_stdout(None):\n",
    "    ...     train_ds, test_ds = get_datasets()\n",
    "    >>>\n",
    "    >>> type(train_ds['image'])\n",
    "    <class 'jaxlib.xla_extension.DeviceArray'>\n",
    "    >>> train_ds['image'].shape\n",
    "    (7291, 16, 16, 1)\n",
    "    >>> train_ds['label'].shape\n",
    "    (7291, 1)\n",
    "    \"\"\"\n",
    "    train_ds = get_dateset(True, 7291)\n",
    "    test_ds = get_dateset(False, 2007)\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMFK51rsAUX4"
   },
   "source": [
    "## 6. Create train state\n",
    "\n",
    "A common pattern in Flax is to create a single dataclass that represents the\n",
    "entire training state, including step number, parameters, and optimizer state.\n",
    "\n",
    "Also adding optimizer & model to this state has the advantage that we only need\n",
    "to pass around a single argument to functions like `train_step()` (see below).\n",
    "\n",
    "Because this is such a common pattern, Flax provides the class\n",
    "[flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/flax.training.html#train-state)\n",
    "that serves most basic usecases. Usually one would subclass it to add more data\n",
    "to be tracked, but in this example we can use it without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QadyBPbWBEAT"
   },
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    cnn = LeCun1989()\n",
    "    params = cnn.init(rng, jnp.ones([1, 16, 16, 1]))[\"params\"]\n",
    "    tx = optax.sgd(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7l-75YE-sr-"
   },
   "source": [
    "## 7. Training step\n",
    "\n",
    "A function that:\n",
    "\n",
    "- Evaluates the neural network given the parameters and a batch of input images\n",
    "  with the\n",
    "  [Module.apply](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)\n",
    "  method.\n",
    "- Computes the `mse_loss` loss function.\n",
    "- Evaluates the loss function and its gradient using\n",
    "  [jax.value_and_grad](https://jax.readthedocs.io/en/latest/jax.html#jax.value_and_grad).\n",
    "- Applies a\n",
    "  [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)\n",
    "  of gradients to the optimizer to update the model's parameters.\n",
    "- Computes the metrics using `compute_metrics` (defined earlier).\n",
    "\n",
    "Use JAX's [@jit](https://jax.readthedocs.io/en/latest/jax.html#jax.jit)\n",
    "decorator to trace the entire `train_step` function and just-in-time compile\n",
    "it with [XLA](https://www.tensorflow.org/xla) into fused device operations\n",
    "that run faster and more efficiently on hardware accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ng11cdMf-z0x"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = LeCun1989().apply({\"params\": params}, batch[\"image\"])\n",
    "        loss = mse_loss(logits=logits, labels=batch[\"label\"])\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=batch[\"label\"])\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-4qDNUgBryr"
   },
   "source": [
    "## 8. Evaluation step\n",
    "\n",
    "Create a function that evaluates your model on the test set with\n",
    "[Module.apply](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1J9i6alBv_u"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    logits = LeCun1989().apply({\"params\": params}, batch[\"image\"])\n",
    "    return compute_metrics(logits=logits, labels=batch[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBTLQPC4BxgH"
   },
   "source": [
    "## 9. Train function\n",
    "\n",
    "Define a training function that:\n",
    "\n",
    "- Shuffles the training data before each epoch using\n",
    "  [jax.random.permutation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.permutation.html)\n",
    "  that takes a PRNGKey as a parameter (check the\n",
    "  [JAX - the sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#JAX-PRNG)).\n",
    "- Runs an optimization step for each batch.\n",
    "- Retrieves the training metrics from the device with `jax.device_get` and\n",
    "  computes their mean across each batch in an epoch.\n",
    "- Returns the optimizer with updated parameters and the training loss and\n",
    "  accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ipyJ-JGCNqP"
   },
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_ds_size = len(train_ds[\"image\"])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {k: np.mean([metrics[k] for metrics in batch_metrics_np]) for k in batch_metrics_np[0]}\n",
    "\n",
    "    print(\n",
    "        \"train epoch: %d, loss: %.4f, accuracy: %.2f\"\n",
    "        % (epoch, epoch_metrics_np[\"loss\"], epoch_metrics_np[\"accuracy\"] * 100)\n",
    "    )\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2cHbVUfCRMv"
   },
   "source": [
    "## 10. Eval function\n",
    "\n",
    "Create a model evaluation function that:\n",
    "\n",
    "- Retrieves the evaluation metrics from the device with `jax.device_get`.\n",
    "- Copies the metrics\n",
    "  [data stored](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables)\n",
    "  in a JAX\n",
    "  [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dKahNmMCr5q"
   },
   "outputs": [],
   "source": [
    "def eval_model(params, test_ds):\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary[\"loss\"], summary[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHQi20yVCsSf"
   },
   "source": [
    "## 11. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CLXnP3KHptR"
   },
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56rKPl6OHqu8"
   },
   "source": [
    "## 12. Seed randomness\n",
    "\n",
    "- Get one\n",
    "  [PRNGKey](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html#jax.random.PRNGKey)\n",
    "  and\n",
    "  [split](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax.random.split)\n",
    "  it to get a second key that you'll use for parameter initialization. (Learn\n",
    "  more about\n",
    "  [PRNG chains](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables)\n",
    "  and\n",
    "  [JAX PRNG design](https://github.com/google/jax/blob/main/design_notes/prng.md).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n53xh_B8Ht_W"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3iHFiAuH41s"
   },
   "source": [
    "## 13. Initialize train state\n",
    "\n",
    "Remember that function initializes both the model parameters and the optimizer\n",
    "and puts both into the training state dataclass that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mj6OfdEEIU-o"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_87fL90dH-0Z"
   },
   "outputs": [],
   "source": [
    "state = create_train_state(init_rng, learning_rate)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can verify that the parameters are in the correct shape."
   ],
   "metadata": {
    "id": "zJ4FERTkZAJl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert state.params[\"H1w\"][\"kernel\"].shape == (5, 5, 1, 12)\n",
    "assert state.params[\"H1b\"][\"bias\"].shape == (8, 8, 12)\n",
    "assert state.params[\"H2s1w\"][\"kernel\"].shape == (5, 5, 8, 4)\n",
    "assert state.params[\"H2b\"][\"bias\"].shape == (4, 4, 12)\n",
    "assert state.params[\"H3\"][\"kernel\"].shape == (4 * 4 * 12, 30)\n",
    "assert state.params[\"H3\"][\"bias\"].shape == (30,)\n",
    "assert state.params[\"Out\"][\"kernel\"].shape == (30, 10)\n",
    "assert state.params[\"Out\"][\"bias\"].shape == (10,)"
   ],
   "metadata": {
    "id": "VhEwUgYxXdsm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqNrWu7kIC9S"
   },
   "source": [
    "## 14. Train and evaluate\n",
    "\n",
    "Once the training and testing is done after 23 epochs, the output should show that your model was able to achieve approximately 95% accuracy. This may not seem very impressive, but remember that this network was from 1989!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nxgS5Z5IsT_"
   },
   "outputs": [],
   "source": [
    "num_epochs = 23\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugGlV3u6Iq1A",
    "outputId": "ce0af30a-ded0-4ffd-826e-1f5007ebbf84"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train epoch: 1, loss: 0.1088, accuracy: 82.57\n",
      " test epoch: 1, loss: 0.08, accuracy: 88.29\n",
      "train epoch: 2, loss: 0.0499, accuracy: 92.90\n",
      " test epoch: 2, loss: 0.07, accuracy: 89.44\n",
      "train epoch: 3, loss: 0.0377, accuracy: 94.46\n",
      " test epoch: 3, loss: 0.06, accuracy: 90.33\n",
      "train epoch: 4, loss: 0.0320, accuracy: 95.49\n",
      " test epoch: 4, loss: 0.05, accuracy: 92.33\n",
      "train epoch: 5, loss: 0.0275, accuracy: 96.09\n",
      " test epoch: 5, loss: 0.05, accuracy: 91.68\n",
      "train epoch: 6, loss: 0.0247, accuracy: 96.58\n",
      " test epoch: 6, loss: 0.05, accuracy: 92.78\n",
      "train epoch: 7, loss: 0.0215, accuracy: 97.02\n",
      " test epoch: 7, loss: 0.04, accuracy: 93.77\n",
      "train epoch: 8, loss: 0.0197, accuracy: 97.28\n",
      " test epoch: 8, loss: 0.05, accuracy: 93.42\n",
      "train epoch: 9, loss: 0.0179, accuracy: 97.54\n",
      " test epoch: 9, loss: 0.05, accuracy: 93.17\n",
      "train epoch: 10, loss: 0.0154, accuracy: 97.94\n",
      " test epoch: 10, loss: 0.05, accuracy: 92.63\n",
      "train epoch: 11, loss: 0.0138, accuracy: 98.26\n",
      " test epoch: 11, loss: 0.05, accuracy: 92.97\n",
      "train epoch: 12, loss: 0.0120, accuracy: 98.49\n",
      " test epoch: 12, loss: 0.04, accuracy: 94.37\n",
      "train epoch: 13, loss: 0.0111, accuracy: 98.56\n",
      " test epoch: 13, loss: 0.04, accuracy: 93.57\n",
      "train epoch: 14, loss: 0.0098, accuracy: 98.77\n",
      " test epoch: 14, loss: 0.04, accuracy: 93.57\n",
      "train epoch: 15, loss: 0.0088, accuracy: 98.86\n",
      " test epoch: 15, loss: 0.04, accuracy: 93.67\n",
      "train epoch: 16, loss: 0.0075, accuracy: 98.99\n",
      " test epoch: 16, loss: 0.04, accuracy: 93.92\n",
      "train epoch: 17, loss: 0.0074, accuracy: 98.97\n",
      " test epoch: 17, loss: 0.04, accuracy: 93.92\n",
      "train epoch: 18, loss: 0.0063, accuracy: 99.15\n",
      " test epoch: 18, loss: 0.04, accuracy: 94.07\n",
      "train epoch: 19, loss: 0.0053, accuracy: 99.27\n",
      " test epoch: 19, loss: 0.04, accuracy: 94.02\n",
      "train epoch: 20, loss: 0.0051, accuracy: 99.25\n",
      " test epoch: 20, loss: 0.04, accuracy: 94.32\n",
      "train epoch: 21, loss: 0.0046, accuracy: 99.36\n",
      " test epoch: 21, loss: 0.04, accuracy: 94.02\n",
      "train epoch: 22, loss: 0.0040, accuracy: 99.41\n",
      " test epoch: 22, loss: 0.04, accuracy: 94.07\n",
      "train epoch: 23, loss: 0.0039, accuracy: 99.47\n",
      " test epoch: 23, loss: 0.04, accuracy: 93.72\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Use a separate PRNG key to permute image data during shuffling\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    # Run an optimization step over a training batch\n",
    "    state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "    # Evaluate on the test set after each training epoch\n",
    "    test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
    "    print(\" test epoch: %d, loss: %.2f, accuracy: %.2f\" % (epoch, test_loss, test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKcRiQ89xQkF"
   },
   "source": [
    "Congrats! You made it to the end of the annotated LeCun1989 example. You can revisit\n",
    "the same example, but structured differently as a couple of Python modules, test\n",
    "modules, config files, another Colab, and documentation in Flax's Git repo:\n",
    "\n",
    "https://github.com/google/flax/tree/main/examples/mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "00KMQd0PaGdJ"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lecun1989_with_commentary",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}