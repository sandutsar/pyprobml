{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Meduri Venkata Shivaditya\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\"\"\"\n",
    "z = Wx + µ + E\n",
    "the equation above represents the latent variable model which \n",
    "relates a d-dimensional data vector z to a corresponding q-dimensional \n",
    "latent variables x \n",
    "with q < d, for isotropic noise E ∼ N (0, σ2I)\n",
    "z : latent\n",
    "x : data\n",
    "W : latent_to_observation matrix\n",
    "µ : centres_of_clusters\n",
    "E : var_of_latent\n",
    "This code is an implementation of generative model of mixture of PPCA\n",
    "Given the number of clusters, data_dim(D) and latent_dim(L)\n",
    "we generate the data for every cluster n,  \n",
    "we sample zn from a Gaussian prior and pass it through the\n",
    "Wk matrix and add noise, where Wk maps from the L-dimensional subspace to the D-dimensional\n",
    "visible space. Using the expectation maximization algorithm we estimate the parameters \n",
    "and then we plot the PC vectors\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mixture_ppca_parameter_initialization(data, n_clusters, latent_dim, n_iterations):\n",
    "    \"\"\"\n",
    "    The k-means algorithm is used to determine the centres. The\n",
    "        priors are computed from the proportion of examples belonging to each\n",
    "        cluster. The covariance matrices are calculated as the sample\n",
    "        covariance of the points associated with (i.e. closest to) the\n",
    "        corresponding centres. For a mixture of PPCA model, the PPCA\n",
    "        decomposition is calculated for the points closest to a given centre.\n",
    "        This initialisation can be used as the starting point for training\n",
    "        the model using the EM algorithm.\n",
    "        W : latent_to_observation matrix\n",
    "    µ/mu : centres_of_clusters\n",
    "    pi : proportion of data in each cluster\n",
    "    sigma2 : variance of latent\n",
    "    covars : covariance of the points associated with (i.e. closest to) the\n",
    "        corresponding centres\n",
    "    \"\"\"\n",
    "    n_datapts, data_dim = data.shape\n",
    "    # initialization of the centres of clusters\n",
    "    init_centers = np.random.randint(0, n_datapts, n_clusters)\n",
    "    # Randomly choose distinct initial centres for the clusters\n",
    "    while len(np.unique(init_centers)) != n_clusters:\n",
    "        init_centers = np.random.randint(0, n_datapts, n_clusters)\n",
    "    mu = data[init_centers, :]\n",
    "    distance_square = np.zeros((n_datapts, n_clusters))\n",
    "    clusters = np.zeros(n_datapts, dtype=np.int32)\n",
    "    # Running iterations for K means algorithm to assign centres for clusters\n",
    "    for k in range(n_iterations):\n",
    "        # assign clusters\n",
    "        for c in range(n_clusters):\n",
    "            distance_square[:, c] = np.power(data - mu[c, :], 2).sum(1)\n",
    "        clusters = np.argmin(distance_square, axis=1)\n",
    "        # compute distortion\n",
    "        distmin = distance_square[range(n_datapts), clusters]\n",
    "        # compute new centers\n",
    "        for c in range(n_clusters):\n",
    "            mu[c, :] = data[clusters == c, :].mean(0)\n",
    "\n",
    "    # parameter initialization\n",
    "    pi = np.zeros(n_clusters)  # Sum should be equal to 1\n",
    "    W = np.zeros((n_clusters, data_dim, latent_dim))\n",
    "    sigma2 = np.zeros(n_clusters)\n",
    "    for c in range(n_clusters):\n",
    "        W[c, :, :] = np.random.randn(data_dim, latent_dim)\n",
    "        pi[c] = (clusters == c).sum() / n_datapts\n",
    "        sigma2[c] = (distmin[clusters == c]).mean() / data_dim\n",
    "    covars = np.zeros(n_clusters)\n",
    "    for i in range(n_clusters):\n",
    "        covars[i] = (np.var(data[clusters == i, 0]) + np.var(data[clusters == i, 1])) / 2\n",
    "    return pi, mu, W, sigma2, covars, clusters\n",
    "\n",
    "\n",
    "def mixture_ppca_expectation_maximization(data, pi, mu, W, sigma2, niter):\n",
    "    \"\"\"\n",
    "    we can find the p(latent|data) with the assumption that data is gaussian\n",
    "    z : latent\n",
    "    x : data\n",
    "    W : latent_to_observation matrix\n",
    "    µ/mu : centres_of_clusters\n",
    "    d : data_dimension\n",
    "    q : latent_dimention\n",
    "    σ2/ sigma2 : variance of latent\n",
    "    π/pi : cluster proportion\n",
    "    p(z|x) = (2πσ2)^−d/2 * exp(−1/(2σ2) * ||z − Wx − µ||)\n",
    "    p(z) = ∫p(z|x)p(x)dx\n",
    "    Solving for p(z) and then using the result we can find the p(x|z)\n",
    "    through which we can find\n",
    "    the log likelihood function which is\n",
    "    log_likelihood = −N/2 * (d ln(2π) + ln |Σ| + tr(Σ−1S))\n",
    "    We can develop an iterative EM algorithm for\n",
    "    optimisation of all of the model parameters µ,W and σ2\n",
    "    If Rn,i = p(zn, i) is the posterior responsibility of\n",
    "    mixture i for generating data point zn,given by\n",
    "    Rn,i = (p(zn|i) * πi) / p(zn)\n",
    "    Using EM, the parameter estimates are as follows:\n",
    "    µi = Σ (Rn,i * zn) / Σ Rn,i\n",
    "    Si = 1/(πi*N) * ΣRn,i*(zn − µi)*(zn − µi)'\n",
    "    Using Si we can estimate W and σ2\n",
    "    For more information on EM algorithm for mixture of PPCA\n",
    "    visit Mixtures of Probabilistic Principal Component Analysers\n",
    "    by Michael E. Tipping and Christopher M. Bishop:\n",
    "    page 5-10 of http://www.miketipping.com/papers/met-mppca.pdf\n",
    "    \"\"\"\n",
    "    n_datapts, data_dim = data.shape\n",
    "    n_clusters = len(sigma2)\n",
    "    _, latent_dim = W[0].shape\n",
    "    M = np.zeros((n_clusters, latent_dim, latent_dim))\n",
    "    Minv = np.zeros((n_clusters, latent_dim, latent_dim))\n",
    "    Cinv = np.zeros((n_clusters, data_dim, data_dim))\n",
    "    logR = np.zeros((n_datapts, n_clusters))\n",
    "    R = np.zeros((n_datapts, n_clusters))\n",
    "    M[:] = 0.0\n",
    "    Minv[:] = 0.0\n",
    "    Cinv[:] = 0.0\n",
    "    log_likelihood = np.zeros(niter)\n",
    "\n",
    "    for i in range(niter):\n",
    "        print(\".\", end=\"\")\n",
    "        for c in range(n_clusters):\n",
    "            # M\n",
    "            \"\"\"\n",
    "            M = σ2I + WT.W\n",
    "            \"\"\"\n",
    "            M[c, :, :] = sigma2[c] * np.eye(latent_dim) + np.dot(W[c, :, :].T, W[c, :, :])\n",
    "\n",
    "            Minv[c, :, :] = np.linalg.inv(M[c, :, :])\n",
    "\n",
    "            # Cinv\n",
    "            Cinv[c, :, :] = (np.eye(data_dim) - np.dot(np.dot(W[c, :, :], Minv[c, :, :]), W[c, :, :].T)) / sigma2[c]\n",
    "\n",
    "            # R_ni\n",
    "            deviation_from_center = data - mu[c, :]\n",
    "            logR[:, c] = (\n",
    "                np.log(pi[c])\n",
    "                + 0.5\n",
    "                * np.log(np.linalg.det(np.eye(data_dim) - np.dot(np.dot(W[c, :, :], Minv[c, :, :]), W[c, :, :].T)))\n",
    "                - 0.5 * data_dim * np.log(sigma2[c])\n",
    "                - 0.5 * (deviation_from_center * np.dot(deviation_from_center, Cinv[c, :, :].T)).sum(1)\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "        Using the log-sum-trick,  visit Section 2.5.4 in \"Probabilistic Machine Learning: An Introduction\" by Kevin P. Murphy for more information\n",
    "        logsumexp(logR - myMax, axis=1) can be replaced by logsumexp(logR, axis=1)\n",
    "        myMax + logsumexp((logR - myMax), axis=0) can be replaced by logsumexp(logR, axis=0)\n",
    "        myMax in the above equations refer to\n",
    "        myMax = logR.max(axis=0) & myMax = logR.max(axis=1).reshape((n_datapts, 1))\n",
    "        \"\"\"\n",
    "        log_likelihood[i] = (logsumexp(logR, axis=1)).sum(axis=0) - n_datapts * data_dim * np.log(2 * math.pi) / 2.0\n",
    "\n",
    "        logR = logR - np.reshape(logsumexp(logR, axis=1), (n_datapts, 1))\n",
    "\n",
    "        logpi = logsumexp(logR, axis=0) - np.log(n_datapts)\n",
    "        logpi = logpi.T\n",
    "        pi = np.exp(logpi)\n",
    "        R = np.exp(logR)\n",
    "        for c in range(n_clusters):\n",
    "            mu[c, :] = (R[:, c].reshape((n_datapts, 1)) * data).sum(axis=0) / R[:, c].sum()\n",
    "            deviation_from_center = data - mu[c, :].reshape((1, data_dim))\n",
    "            \"\"\"\n",
    "            Si = 1/(πi*N) * ΣRn,i*(zn − µi)*(zn − µi)'\n",
    "            Si is used to estimate \n",
    "            \"\"\"\n",
    "            Si = (1 / (pi[c] * n_datapts)) * np.dot(\n",
    "                (R[:, c].reshape((n_datapts, 1)) * deviation_from_center).T, np.dot(deviation_from_center, W[c, :, :])\n",
    "            )\n",
    "\n",
    "            Wnew = np.dot(\n",
    "                Si, np.linalg.inv(sigma2[c] * np.eye(latent_dim) + np.dot(np.dot(Minv[c, :, :], W[c, :, :].T), Si))\n",
    "            )\n",
    "\n",
    "            sigma2[c] = (1 / data_dim) * (\n",
    "                (R[:, c].reshape(n_datapts, 1) * np.power(deviation_from_center, 2)).sum() / (n_datapts * pi[c])\n",
    "                - np.trace(np.dot(np.dot(Si, Minv[c, :, :]), Wnew.T))\n",
    "            )\n",
    "\n",
    "            W[c, :, :] = Wnew\n",
    "\n",
    "    return pi, mu, W, sigma2, log_likelihood\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    n = 500\n",
    "    r = np.random.rand(1, n) + 1\n",
    "    theta = np.random.rand(1, n) * (2 * math.pi)\n",
    "    x1 = r * np.sin(theta)\n",
    "    x2 = r * np.cos(theta)\n",
    "    X = np.vstack((x1, x2))\n",
    "    return np.transpose(X)\n",
    "\n",
    "\n",
    "def mixppcademo(data, n_clusters):\n",
    "    \"\"\"\n",
    "    W : latent to observation matrix\n",
    "    mu : centres_of_clusters\n",
    "    pi : proportions of data in each of the cluster\n",
    "    sigma2 : variance of latent\n",
    "    L : log likelihood after each iteration\n",
    "    covars : covariance of the points associated with (i.e. closest to) the\n",
    "        corresponding centres\n",
    "    \"\"\"\n",
    "    plt.plot(data[:, 0], data[:, 1], \"o\", c=\"blue\", mfc=\"none\")\n",
    "    pi, mu, W, sigma2, covars, clusters = mixture_ppca_parameter_initialization(\n",
    "        data, n_clusters, latent_dim=1, n_iterations=10\n",
    "    )\n",
    "    pi, mu, W, sigma2, L = mixture_ppca_expectation_maximization(data, pi, mu, W, sigma2, 10)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        v = W[i, :, :]\n",
    "        # Plotting the pc vectors using 2 standard deviations\n",
    "        start = mu[i].reshape((2, 1)) - (v * 2 * np.sqrt(sigma2[i]))\n",
    "        endpt = mu[i].reshape((2, 1)) + (v * 2 * np.sqrt(sigma2[i]))\n",
    "        linex = [start[0], endpt[0]]\n",
    "        liney = [start[1], endpt[1]]\n",
    "        plt.plot(linex, liney, linewidth=3, c=\"black\")\n",
    "        theta = np.arange(0, 2 * math.pi, 0.02)\n",
    "        # Plotting the confidence interval ellipse using 2 standard deviations\n",
    "        x = 2 * np.sqrt(sigma2[i]) * np.cos(theta)\n",
    "        y = np.sqrt(covars[i]) * np.sin(theta)\n",
    "        rot_matrix = np.vstack((np.hstack((v[0], -v[1])), np.hstack((v[1], v[0]))))\n",
    "        ellipse = np.dot(rot_matrix, np.vstack((x, y)))\n",
    "        ellipse = np.transpose(ellipse)\n",
    "        ellipse = ellipse + np.dot(np.ones((len(theta), 1)), mu[i, :].reshape((1, 2)))\n",
    "        plt.plot(ellipse[:, 0], ellipse[:, 1], c=\"crimson\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    np.random.seed(61)\n",
    "    data = generate_data()\n",
    "    plt.figure(0)\n",
    "    mixppcademo(data, n_clusters=1)\n",
    "    plt.savefig(\"mixppca_k-1.png\", dpi=300)\n",
    "    np.random.seed(7)\n",
    "    data = generate_data()\n",
    "    plt.figure(1)\n",
    "    mixppcademo(data, n_clusters=10)\n",
    "    plt.savefig(\"mixppca_k-10.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
