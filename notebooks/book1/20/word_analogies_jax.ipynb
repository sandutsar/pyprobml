{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find torch implementation of this notebook here: https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/book1/20/word_analogies_torch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/word_analogies_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NznbOHRe2fO"
   },
   "source": [
    "\n",
    "# Solving word analogies using pre-trained word embeddings\n",
    "\n",
    "Based on D2L 14.7\n",
    "\n",
    "http://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7FMeymiael2A"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import requests\n",
    "import zipfile\n",
    "import hashlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Pa-hUPXw4DmL"
   },
   "outputs": [],
   "source": [
    "# Required functions\n",
    "def download(name, cache_dir=os.path.join(\"..\", \"data\")):\n",
    "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split(\"/\")[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # Hit cache\n",
    "    print(f\"Downloading {fname} from {url}...\")\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def download_extract(name, folder=None):\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == \".zip\":\n",
    "        fp = zipfile.ZipFile(fname, \"r\")\n",
    "    elif ext in (\".tar\", \".gz\"):\n",
    "        fp = tarfile.open(fname, \"r\")\n",
    "    else:\n",
    "        assert False, \"Only zip/tar files can be extracted.\"\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1xTDK1Qf9aA"
   },
   "source": [
    "# Get pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0GSs7iSfuH7"
   },
   "source": [
    "Pretrained embeddings taken from \n",
    "\n",
    "GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "fastText website: https://fasttext.cc/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w1NzVIFPfBFQ"
   },
   "outputs": [],
   "source": [
    "DATA_HUB = dict()\n",
    "DATA_URL = \"http://d2l-data.s3-accelerate.amazonaws.com/\"\n",
    "\n",
    "DATA_HUB[\"glove.6b.50d\"] = (DATA_URL + \"glove.6B.50d.zip\", \"0b8703943ccdb6eb788e6f091b8946e82231bc4d\")\n",
    "\n",
    "DATA_HUB[\"glove.6b.100d\"] = (DATA_URL + \"glove.6B.100d.zip\", \"cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a\")\n",
    "\n",
    "DATA_HUB[\"glove.42b.300d\"] = (DATA_URL + \"glove.42B.300d.zip\", \"b5116e234e9eb9076672cfeabf5469f3eec904fa\")\n",
    "\n",
    "DATA_HUB[\"wiki.en\"] = (DATA_URL + \"wiki.en.zip\", \"c1816da3821ae9f43899be655002f6c723e91b88\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XZR0gKx0f2im"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = [\"<unk>\"], []\n",
    "        # data_dir = d2l.download_extract(embedding_name)\n",
    "        data_dir = download_extract(embedding_name)\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, \"vec.txt\"), \"r\") as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(\" \")\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, jnp.array(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx) for token in tokens]\n",
    "        vecs = self.idx_to_vec[jnp.array(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX6p18D3gHKT"
   },
   "source": [
    "Get a 50dimensional glove embedding, with vocab size of 400k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8A_9mY5qgGbZ",
    "outputId": "c7f76a90-b5b3-4105-8002-75fb04a47e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/glove.6B.50d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip...\n"
     ]
    }
   ],
   "source": [
    "glove_6b50d = TokenEmbedding(\"glove.6b.50d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgmGyJStgGo_",
    "outputId": "1c85c208-76dc-4a63-a6b8-9ab89b67d959"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrWGMzT3gQpi"
   },
   "source": [
    "Map from word to index and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvnnAN5vgNj2",
    "outputId": "6a27dd59-b520-4418-b7c5-956c8cefcfb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3367, 'beautiful')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_6b50d.token_to_idx[\"beautiful\"], glove_6b50d.idx_to_token[3367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D-tiXFE0kJX8"
   },
   "outputs": [],
   "source": [
    "embedder = glove_6b50d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z5F8ltlBhTAi"
   },
   "outputs": [],
   "source": [
    "# embedder = TokenEmbedding('glove.6b.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9Gs8Vdzka6H",
    "outputId": "94e1d8c3-5c6f-46ab-b3b8-9fb8c91278fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.idx_to_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvItt6znggX0"
   },
   "source": [
    "# Finding most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ve2x_2aXgQDD"
   },
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    # The added 1e-9 is for numerical stability\n",
    "    cos = (W @ x.reshape(-1, 1)).reshape(-1) / ((jnp.sqrt(jnp.sum(W * W, axis=1) + 1e-9) * jnp.sqrt((x * x).sum())))\n",
    "    _, topk = jax.lax.top_k(cos, k=k)\n",
    "    return topk, [cos[int(i)] for i in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3jRdTj2rgibv"
   },
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):  # Remove input words\n",
    "        print(f\"cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3S5rJ20Lgltu",
    "outputId": "1b1ff7dc-ea91-4410-edb9-553655f5cfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.886: woman\n",
      "cosine sim=0.856: boy\n",
      "cosine sim=0.845: another\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens(\"man\", 3, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-9JlDAwgsVJ",
    "outputId": "4b53ffe4-a20c-47f8-88bd-e86c7a2b3968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.815: bananas\n",
      "cosine sim=0.787: coconut\n",
      "cosine sim=0.758: pineapple\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens(\"banana\", 3, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRmfkSHdgyTg"
   },
   "source": [
    "# Word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qfmQvY-zhx5L"
   },
   "outputs": [],
   "source": [
    "# We slightly modify D2L code so it works on the man:woman:king:queen example\n",
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    vecs = embed[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx_to_vec, x, 10)\n",
    "    # remove word c from nearest neighbor\n",
    "    idx_c = embed.token_to_idx[token_c]\n",
    "    topk = list(topk)\n",
    "    topk.remove(idx_c)\n",
    "    return embed.idx_to_token[int(topk[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "n8BTBz0Qg5ZL",
    "outputId": "c8156f4f-fbad-4d0f-d177-eadbb0020a23"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy(\"man\", \"woman\", \"king\", embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GroF4uZ9g28K",
    "outputId": "599abb1a-6b8c-4ce6-b3bb-933377dcf7fb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy(\"man\", \"woman\", \"son\", embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6cQyWtLzhHSu",
    "outputId": "9e937da5-b2f2-4798-e7fd-0bdfa3a512ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'japan'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy(\"beijing\", \"china\", \"tokyo\", embedder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_analogies_jax.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "0c4db7afc41426cdbd40f1ab6c1e8c3d7a2ad67aa0e910a9ae65a81b500bda87"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "dfc3641d2968f953b738b1dc0c9747e3c41396f999ba0376896eb7547ee9da03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
