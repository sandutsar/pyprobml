{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-armed bandit problem for a linear Gaussian model\n",
    "# with linear reward function.\n",
    "# In this demo, we consider three arms:\n",
    "# 1. The first arm is an upward-trending arm with initial negative bias\n",
    "# 2. The second arm is a downward-trending arm with initial positive bias\n",
    "# 3. The third arm is a stationary arm with initial zero bias\n",
    "# !pip install -qq -Uq tfp-nightly[jax] > /dev/null\n",
    "\n",
    "# Author: Gerardo Durán-Martín (@gerdm)\n",
    "\n",
    "\n",
    "import jax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from jax import random\n",
    "from functools import partial\n",
    "from jax.nn import one_hot\n",
    "\n",
    "try:\n",
    "    from tensorflow_probability.substrates import jax as tfp\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq tensorflow-probability\n",
    "    from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class NormalGammaBandit:\n",
    "    def sample(self, key, params, state):\n",
    "        key_sigma, key_w = random.split(key, 2)\n",
    "        sigma2_samp = tfd.InverseGamma(concentration=params[\"a\"], scale=params[\"b\"]).sample(seed=key_sigma)\n",
    "        cov_matrix_samples = sigma2_samp[:, None, None] * params[\"Sigma\"]\n",
    "        w_samp = tfd.MultivariateNormalFullCovariance(loc=params[\"mu\"], covariance_matrix=cov_matrix_samples).sample(\n",
    "            seed=key_w\n",
    "        )\n",
    "        return sigma2_samp, w_samp\n",
    "\n",
    "    def predict_rewards(self, params_sample, state):\n",
    "        sigma2_samp, w_samp = params_sample\n",
    "        predicted_reward = jnp.einsum(\"m,km->k\", state, w_samp)\n",
    "        return predicted_reward\n",
    "\n",
    "    def update(self, action, params, state, reward):\n",
    "        \"\"\"\n",
    "        Update the parameters of the model for the\n",
    "        chosen arm\n",
    "        \"\"\"\n",
    "        mu_k = params[\"mu\"][action]\n",
    "        Sigma_k = params[\"Sigma\"][action]\n",
    "        Lambda_k = jnp.linalg.inv(Sigma_k)\n",
    "        a_k = params[\"a\"][action]\n",
    "        b_k = params[\"b\"][action]\n",
    "\n",
    "        # weight params\n",
    "        Lambda_update = jnp.outer(state, state) + Lambda_k\n",
    "        Sigma_update = jnp.linalg.inv(Lambda_update)\n",
    "        mu_update = Sigma_update @ (Lambda_k @ mu_k + state * reward)\n",
    "        # noise params\n",
    "        a_update = a_k + 1 / 2\n",
    "        b_update = b_k + (reward**2 + mu_k.T @ Lambda_k @ mu_k - mu_update.T @ Lambda_update @ mu_update) / 2\n",
    "\n",
    "        # Update only the chosen action at time t\n",
    "        params[\"mu\"] = params[\"mu\"].at[action].set(mu_update)\n",
    "        params[\"Sigma\"] = params[\"Sigma\"].at[action].set(Sigma_update)\n",
    "        params[\"a\"] = params[\"a\"].at[action].set(a_update)\n",
    "        params[\"b\"] = params[\"b\"].at[action].set(b_update)\n",
    "\n",
    "        params = {\"mu\": params[\"mu\"], \"Sigma\": params[\"Sigma\"], \"a\": params[\"a\"], \"b\": params[\"b\"]}\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "def true_reward(key, action, state, true_params):\n",
    "    \"\"\"\n",
    "    Compute true reward as the linear combination\n",
    "    of each set of weights and the observed state plus\n",
    "    the noise from each arm\n",
    "    \"\"\"\n",
    "    w_k = true_params[\"w\"][action]\n",
    "    sigma_k = jnp.sqrt(true_params[\"sigma2\"][action])\n",
    "    reward = w_k @ state + random.normal(key) * sigma_k\n",
    "    return reward\n",
    "\n",
    "\n",
    "def thompson_sampling_step(model_params, state, model, environment):\n",
    "    \"\"\"\n",
    "    Contextual implementation of the Thompson sampling algorithm.\n",
    "    This implementation considers a single step\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_params: dict\n",
    "    environment: function\n",
    "    key: jax.random.PRNGKey\n",
    "    moidel: instance of a Bandit model\n",
    "    \"\"\"\n",
    "    key, context = state\n",
    "    key_sample, key_reward = random.split(key)\n",
    "    # Sample an choose an action\n",
    "    params = model.sample(key_sample, model_params, context)\n",
    "    pred_rewards = model.predict_rewards(params, context)\n",
    "    action = pred_rewards.argmax()\n",
    "    # environment reward\n",
    "    reward = environment(key_reward, action, context)\n",
    "    model_params = model.update(action, model_params, context, reward)\n",
    "\n",
    "    arm_reward = one_hot(action, K) * reward\n",
    "    return model_params, (model_params, arm_reward)\n",
    "\n",
    "\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "\n",
    "# 1. Specify underlying dynamics (unknown)\n",
    "W = jnp.array([[-5.0, 2.0, 0.5], [0.0, 0.0, 0.0], [5.0, -1.5, -1.0]])\n",
    "\n",
    "sigmas = jnp.ones(3)\n",
    "\n",
    "K, M = W.shape\n",
    "N = 500\n",
    "T = 4\n",
    "x = jnp.linspace(0, T, N)\n",
    "X = jnp.c_[jnp.ones(N), x, x**2]\n",
    "\n",
    "true_params = {\"w\": W, \"sigma2\": sigmas**2}\n",
    "\n",
    "\n",
    "# 2. Sample one instance of the multi-armed bandit process\n",
    "#    this is only for plotting, it will not be used fo training\n",
    "key = random.PRNGKey(314)\n",
    "noise = random.multivariate_normal(key, mean=jnp.zeros(K), cov=jnp.eye(K) * sigmas, shape=(N,))\n",
    "Y = jnp.einsum(\"nm,km->nk\", X, W) + noise\n",
    "\n",
    "\n",
    "# 3. Configure the model parameters that will be used\n",
    "# during Thompson sampling\n",
    "eta = 2.0\n",
    "lmbda = 5.0\n",
    "init_params = {\n",
    "    \"mu\": jnp.zeros((K, M)),\n",
    "    \"Sigma\": lmbda * jnp.eye(M) * jnp.ones((K, 1, 1)),\n",
    "    \"a\": eta * jnp.ones(K),\n",
    "    \"b\": eta * jnp.ones(K),\n",
    "}\n",
    "environment = partial(true_reward, true_params=true_params)\n",
    "thompson_partial = partial(thompson_sampling_step, model=NormalGammaBandit(), environment=environment)\n",
    "thompson_vmap = jax.vmap(lambda key: jax.lax.scan(thompson_partial, init_params, (random.split(key, N), X)))\n",
    "\n",
    "\n",
    "# 4. Do Thompson sampling\n",
    "nsamples = 100\n",
    "key = random.PRNGKey(3141)\n",
    "keys = random.split(key, nsamples)\n",
    "posteriors_samples, (_, hist_reward_samples) = thompson_vmap(keys)\n",
    "\n",
    "\n",
    "# 5. Plotting\n",
    "# 5.1 Example dataset\n",
    "plt.plot(x, Y)\n",
    "plt.axhline(y=0, c=\"black\")\n",
    "plt.legend([f\"arm{i}\" for i in range(K)])\n",
    "pml.savefig(\"bandit-lingauss-true-reward.pdf\")\n",
    "\n",
    "# 5.2 Plot heatmap of chosen arm and given reward\n",
    "ix = 0\n",
    "map_reward = hist_reward_samples[ix]\n",
    "map_reward = map_reward.at[map_reward == 0].set(jnp.nan)\n",
    "labels = [f\"arm{i}\" for i in range(K)]\n",
    "map_reward_df = pd.DataFrame(map_reward, index=[f\"{t:0.2f}\" for t in x], columns=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 5))\n",
    "sns.heatmap(map_reward_df, cmap=\"viridis\", ax=ax, xticklabels=labels)\n",
    "plt.ylabel(\"time\")\n",
    "pml.savefig(\"bandit-lingauss-heatmap.pdf\")\n",
    "\n",
    "# 5.3 Plot cumulative reward per arm\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x, hist_reward_samples[ix].cumsum(axis=0))\n",
    "plt.legend(labels, loc=\"upper left\")\n",
    "plt.ylabel(\"cumulative reward\")\n",
    "plt.xlabel(\"time\")\n",
    "pml.savefig(\"bandit-lingauss-cumulative-reward.pdf\")\n",
    "\n",
    "# 5.4 Plot regret\n",
    "fig, ax = plt.subplots()\n",
    "expected_hist_reward = hist_reward_samples.mean(axis=0)\n",
    "optimal_reward = jnp.einsum(\"nm,km->nk\", X, true_params[\"w\"]).max(axis=1)\n",
    "regret = optimal_reward - expected_hist_reward.max(axis=1)\n",
    "cumulative_regret = regret.cumsum()\n",
    "\n",
    "\n",
    "# plt.plot(x, cumulative_regret)\n",
    "plt.plot(x, cumulative_regret, label=\"observed\")\n",
    "scale_factor = 20  # empirical\n",
    "plt.plot(x, scale_factor * jnp.sqrt(x), label=\"c $\\sqrt{t}$\")\n",
    "plt.title(\"Cumulative regret\")\n",
    "plt.ylabel(\"$L_T$\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.legend()\n",
    "pml.savefig(\"bandit-lingauss-cumulative-regret.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
